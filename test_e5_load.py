# test_e5_load.py
import os
import torch
from transformers import AutoTokenizer, AutoModel


def test_model_loading():
    model_path = "./models/e5/e5-small-v2"

    print("üß™ –¢–µ—Å—Ç–∏—Ä—É—é –∑–∞–≥—Ä—É–∑–∫—É E5 –º–æ–¥–µ–ª–∏...")
    print(f"üìÅ –ü—É—Ç—å: {model_path}")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–æ–≤
    required_files = ["pytorch_model.bin", "config.json", "tokenizer.json"]
    for file in required_files:
        filepath = os.path.join(model_path, file)
        if os.path.exists(filepath):
            size = os.path.getsize(filepath) / 1024 / 1024
            print(f"‚úÖ {file}: {size:.1f} MB")
        else:
            print(f"‚ùå {file}: –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
            return False

    # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å
    try:
        print("\nüîß –ó–∞–≥—Ä—É–∂–∞—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä...")
        tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
        print("‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω")

        print("üîß –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å...")
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"   –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

        model = AutoModel.from_pretrained(model_path, local_files_only=True).to(device)
        model.eval()

        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")

        # –¢–µ—Å—Ç–æ–≤–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
        print("\nüß™ –¢–µ—Å—Ç–æ–≤–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ...")
        texts = ["Hello world", "What is artificial intelligence?"]

        # E5 —Ç—Ä–µ–±—É–µ—Ç –ø—Ä–µ—Ñ–∏–∫—Å—ã
        texts_with_prefix = [f"query: {text}" for text in texts]

        inputs = tokenizer(
            texts_with_prefix, padding=True, truncation=True, max_length=512, return_tensors="pt"
        ).to(device)

        with torch.no_grad():
            outputs = model(**inputs)

        # Mean pooling
        token_embeddings = outputs.last_hidden_state
        attention_mask = inputs["attention_mask"]
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        embeddings = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(
            input_mask_expanded.sum(1), min=1e-9
        )

        print(f"‚úÖ –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ!")
        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {embeddings.shape}")
        print(f"   –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

        return True

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        return False


if __name__ == "__main__":
    success = test_model_loading()
    if success:
        print("\nüéâ –í—Å—ë –≥–æ—Ç–æ–≤–æ! –¢–µ–ø–µ—Ä—å –∑–∞–ø—É—Å—Ç–∏—Ç–µ —Ç–µ—Å—Ç –∫–∞—Å–∫–∞–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞:")
        print("python test_rag.py")
    else:
        print("\n‚ö†Ô∏è  –ï—Å—Ç—å –ø—Ä–æ–±–ª–µ–º—ã —Å –º–æ–¥–µ–ª—å—é")
