"""
E5 Ð¼Ð¾Ð´ÑƒÐ»ÑŒ Ð´Ð»Ñ TensorFlow (Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð±ÐµÐ· PyTorch DLL Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼)
"""

import numpy as np
from typing import List, Dict, Any, Optional
import logging
import os

logger = logging.getLogger(__name__)

# ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ð±ÑÐºÐµÐ½Ð´
try:
    import torch
    import torch.nn.functional as F
    from transformers import AutoTokenizer, AutoModel

    TORCH_AVAILABLE = True
    logger.info("âœ… PyTorch Ð´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ ÐµÐ³Ð¾")
except ImportError:
    TORCH_AVAILABLE = False
    logger.info("âš ï¸  PyTorch Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½, Ð¿ÐµÑ€ÐµÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ÑÑ Ð½Ð° TensorFlow")

if not TORCH_AVAILABLE:
    import tensorflow as tf
    from transformers import TFAutoModel, AutoTokenizer

    logger.info("âœ… TensorFlow Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½")


class E5Module:
    """E5 ÑÐµÐ¼Ð°Ð½Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð¿Ð¾Ð¸ÑÐº Ñ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ¾Ð¹ PyTorch/TensorFlow"""

    def __init__(
        self,
        name: str = "e5",
        model_path: str = "./models/e5/e5-small-v2",
        bm25_module_name: str = "bm25",
        top_k_candidates: int = 100,
        device: Optional[str] = None,
    ):

        self.name = name
        self.model_path = model_path
        self.bm25_module_name = bm25_module_name
        self.top_k_candidates = top_k_candidates
        self.device = device

        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°: {model_path}")

        # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ
        self._load_model()

        # ÐšÑÑˆ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð²
        self.doc_embeddings = {}

        logger.info(
            f"Ð¡Ð¾Ð·Ð´Ð°Ð½ E5 Ð¼Ð¾Ð´ÑƒÐ»ÑŒ '{name}' (Ð±ÑÐºÐµÐ½Ð´: {'PyTorch' if TORCH_AVAILABLE else 'TensorFlow'})"
        )

    def _load_model(self):
        """Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ñ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¼ Ð²Ñ‹Ð±Ð¾Ñ€Ð¾Ð¼ Ð±ÑÐºÐµÐ½Ð´Ð°"""
        logger.info(f"Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ E5 Ð¸Ð· {self.model_path}")

        try:
            if TORCH_AVAILABLE:
                # PyTorch Ð²ÐµÑ€ÑÐ¸Ñ
                import torch

                self.tokenizer = AutoTokenizer.from_pretrained(
                    self.model_path, local_files_only=True
                )
                self.model = AutoModel.from_pretrained(self.model_path, local_files_only=True)
                if self.device:
                    self.model = self.model.to(self.device)
                else:
                    self.device = "cuda" if torch.cuda.is_available() else "cpu"
                    self.model = self.model.to(self.device)

                self.model.eval()
                logger.info(f"âœ… PyTorch Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð° Ð½Ð° {self.device}")

            else:
                # TensorFlow Ð²ÐµÑ€ÑÐ¸Ñ
                import tensorflow as tf

                self.tokenizer = AutoTokenizer.from_pretrained(
                    self.model_path, local_files_only=True
                )
                self.model = TFAutoModel.from_pretrained(
                    self.model_path,
                    local_files_only=True,
                    from_pt=True,  # ÐšÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð²ÐµÑÐ° PyTorch â†’ TensorFlow
                )

                # Ð”Ð»Ñ TensorFlow Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ GPU ÐµÑÐ»Ð¸ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¾
                gpus = tf.config.list_physical_devices("GPU")
                if gpus:
                    self.device = "gpu"
                    logger.info(f"âœ… TensorFlow Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð°, GPU Ð´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½")
                else:
                    self.device = "cpu"
                    logger.info(f"âœ… TensorFlow Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð° Ð½Ð° CPU")

        except Exception as e:
            logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð¼Ð¾Ð´ÐµÐ»Ð¸: {e}")
            raise

    def _encode_text(self, text: str, is_query: bool = False) -> np.ndarray:
        """ÐšÐ¾Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ñ‚ÐµÐºÑÑ‚Ð° Ð² ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³"""
        # E5 Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ Ð¿Ñ€ÐµÑ„Ð¸ÐºÑÑ‹
        if is_query:
            text = f"query: {text}"
        else:
            text = f"passage: {text}"

        if TORCH_AVAILABLE:
            # PyTorch Ð²ÐµÑ€ÑÐ¸Ñ
            import torch

            inputs = self.tokenizer(
                text, padding=True, truncation=True, max_length=512, return_tensors="pt"
            ).to(self.device)

            with torch.no_grad():
                outputs = self.model(**inputs)

            # Mean pooling
            token_embeddings = outputs.last_hidden_state
            attention_mask = inputs["attention_mask"]
            input_mask_expanded = (
                attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
            )
            embeddings = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(
                input_mask_expanded.sum(1), min=1e-9
            )

            # ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ
            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)

            return embeddings.cpu().numpy()[0]

        else:
            # TensorFlow Ð²ÐµÑ€ÑÐ¸Ñ
            import tensorflow as tf

            inputs = self.tokenizer(
                text,
                padding=True,
                truncation=True,
                max_length=512,
                return_tensors="tf",  # Ð’Ð°Ð¶Ð½Ð¾: return_tensors="tf"
            )

            outputs = self.model(inputs)

            # Mean pooling Ð´Ð»Ñ TensorFlow
            token_embeddings = outputs.last_hidden_state
            attention_mask = tf.cast(inputs["attention_mask"], tf.float32)
            input_mask_expanded = tf.expand_dims(attention_mask, -1)
            input_mask_expanded = tf.broadcast_to(input_mask_expanded, tf.shape(token_embeddings))

            sum_embeddings = tf.reduce_sum(token_embeddings * input_mask_expanded, axis=1)
            sum_mask = tf.reduce_sum(input_mask_expanded, axis=1)
            sum_mask = tf.clip_by_value(sum_mask, 1e-9, tf.float32.max)

            embeddings = sum_embeddings / sum_mask

            # ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ L2
            embeddings = tf.math.l2_normalize(embeddings, axis=1)

            return embeddings.numpy()[0]

    # ÐžÐ¡Ð¢ÐÐ›Ð¬ÐÐ«Ð• ÐœÐ•Ð¢ÐžÐ”Ð« Ð‘Ð•Ð— Ð˜Ð—ÐœÐ•ÐÐ•ÐÐ˜Ð™:
    # search(), add_documents(), _get_bm25_module() Ð¸ Ñ‚.Ð´.
    # ÐžÐ½Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽÑ‚ self._encode_text() ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ñ‚ÐµÐ¿ÐµÑ€ÑŒ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ñ Ð¾Ð±Ð¾Ð¸Ð¼Ð¸ Ð±ÑÐºÐµÐ½Ð´Ð°Ð¼Ð¸

    def search(self, query: str, top_k: int = 5, **kwargs) -> List[Dict[str, Any]]:
        """ÐšÐ°ÑÐºÐ°Ð´Ð½Ñ‹Ð¹ Ð¿Ð¾Ð¸ÑÐº: BM25 â†’ E5"""
        from src.core.rag import rag_engine

        # 1. ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ BM25 Ð¼Ð¾Ð´ÑƒÐ»ÑŒ
        bm25_module = rag_engine.manager.search_modules.get(self.bm25_module_name)
        if not bm25_module:
            logger.error(f"BM25 Ð¼Ð¾Ð´ÑƒÐ»ÑŒ '{self.bm25_module_name}' Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½")
            return []

        # 2. BM25: Ð±Ñ‹ÑÑ‚Ñ€Ñ‹Ð¹ Ð¿Ð¾Ð¸ÑÐº ÐºÐ°Ð½Ð´Ð¸Ð´Ð°Ñ‚Ð¾Ð²
        bm25_results = bm25_module.search(query, top_k=self.top_k_candidates)

        if not bm25_results:
            return []

        logger.info(
            f"ðŸ“Š BM25 Ð½Ð°ÑˆÐµÐ» {len(bm25_results)} ÐºÐ°Ð½Ð´Ð¸Ð´Ð°Ñ‚Ð¾Ð², {('PyTorch' if TORCH_AVAILABLE else 'TensorFlow')} Ð¿ÐµÑ€ÐµÑ€Ð°Ð½Ð¶Ð¸Ñ€ÑƒÐµÑ‚..."
        )

        # 3. E5: ÐºÐ¾Ð´Ð¸Ñ€ÑƒÐµÐ¼ Ð·Ð°Ð¿Ñ€Ð¾Ñ
        query_embedding = self._encode_text(query, is_query=True)

        # 4. ÐŸÐµÑ€ÐµÑ€Ð°Ð½Ð¶Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ
        reranked = []
        for candidate in bm25_results:
            doc_id = candidate.get("id")
            doc_text = candidate.get("content", "")

            # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð¸Ð»Ð¸ Ð²Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÐ¼ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³
            if doc_id in self.doc_embeddings:
                doc_embedding = self.doc_embeddings[doc_id]
            else:
                doc_embedding = self._encode_text(doc_text, is_query=False)
                self.doc_embeddings[doc_id] = doc_embedding

            # ÐšÐ¾ÑÐ¸Ð½ÑƒÑÐ½Ð¾Ðµ ÑÑ…Ð¾Ð´ÑÑ‚Ð²Ð¾
            similarity = float(np.dot(query_embedding, doc_embedding))

            # ÐšÐ¾Ð¼Ð±Ð¸Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ score
            bm25_score = candidate.get("score", 0)
            e5_score = (similarity + 1) / 2  # [-1,1] â†’ [0,1]

            # Ð’ÐµÑÐ°: 40% BM25 + 60% E5
            combined_score = 0.4 * bm25_score + 0.6 * e5_score

            reranked.append(
                {
                    "id": doc_id,
                    "content": doc_text,
                    "score": combined_score,
                    "bm25_score": bm25_score,
                    "e5_score": e5_score,
                    "e5_similarity": similarity,
                    "module": self.name,
                    "backend": "pytorch" if TORCH_AVAILABLE else "tensorflow",
                }
            )

        # 5. Ð¡Ð¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²ÐºÐ° Ð¸ Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ
        reranked.sort(key=lambda x: x["score"], reverse=True)

        if reranked:
            scores = [r["score"] for r in reranked]
            max_score = max(scores) if max(scores) > 0 else 1.0
            for r in reranked:
                r["score"] = r["score"] / max_score

        logger.info(
            f"âœ… {('PyTorch' if TORCH_AVAILABLE else 'TensorFlow')} Ð²ÐµÑ€Ð½ÑƒÐ» {len(reranked[:top_k])} Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð²"
        )

        return reranked[:top_k]

    def clear(self):
        self.doc_embeddings.clear()
        return {"status": "cleared", "name": self.name}

    def get_info(self) -> Dict[str, Any]:
        return {
            "type": "e5",
            "name": self.name,
            "backend": "pytorch" if TORCH_AVAILABLE else "tensorflow",
            "model_path": self.model_path,
            "bm25_source": self.bm25_module_name,
            "device": self.device,
            "embeddings_cached": len(self.doc_embeddings),
        }
