"""
bm25_module.py - BM25 –º–æ–¥—É–ª—å –¥–ª—è –ø–æ–∏—Å–∫–∞ (–º—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã–π)
"""

import numpy as np
from typing import List, Dict, Any, Optional
import json
import pickle
import os
import nltk
from src.core.base import BaseSearchModule


class BM25Module(BaseSearchModule):

    def __init__(self, name: str = "bm25", language: str = "multilingual"):
        self.name = name
        self.language = language
        self.is_fitted = False
        self.documents = []
        self.total_terms = 0
        self.ids = []
        self.bm25 = None

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —è–∑—ã–∫–æ–≤
        nltk.download("punkt", quiet=True)
        nltk.download("stopwords", quiet=True)
        from nltk.corpus import stopwords

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏–∑ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–æ–≤
        self.stop_words = set()
        for lang in ["english", "russian", "french", "spanish", "german"]:
            try:
                self.stop_words.update(stopwords.words(lang))
            except:
                pass

        # –£–±–∏—Ä–∞–µ–º –≤–∞–∂–Ω—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Å—Ç–æ–ø-—Å–ª–æ–≤
        important_words = {
            "war",
            "world",
            "technology",
            "python",
            "intelligence",
            "bitcoin",
            "blockchain",
            "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π",
            "–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç",
            "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è",
        }
        self.stop_words = {w for w in self.stop_words if w not in important_words}

    def _preprocess_text(self, text: str) -> List[str]:
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–µ–∑ —Å—Ç–µ–º–º–∏–Ω–≥–∞"""
        try:
            tokens = nltk.word_tokenize(text.lower())
        except:
            import re

            tokens = re.findall(r"\b\w+\b", text.lower())

        # –ë–æ–ª–µ–µ –º—è–≥–∫–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
        processed = []
        for token in tokens:
            if len(token) > 1 and token not in self.stop_words and not token.isdigit():
                processed.append(token)

        return processed

    def add_documents(self, documents: List[str], ids: Optional[List[str]] = None) -> Dict:
        """–î–æ–±–∞–≤–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ —Å—Ç—Ä–æ–∏—Ç –∏–Ω–¥–µ–∫—Å"""
        if ids is None:
            ids = [
                f"{self.name}_{i}"
                for i in range(len(self.documents), len(self.documents) + len(documents))
            ]

        # –ï—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã —É–∂–µ –µ—Å—Ç—å, –æ—á–∏—â–∞–µ–º
        if self.documents:
            self.documents = []
            self.ids = []

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        self.documents.extend(documents)
        self.ids.extend(ids)

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        processed_docs = [self._preprocess_text(doc) for doc in self.documents]

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –µ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        if not processed_docs or all(len(doc) == 0 for doc in processed_docs):
            print(f"‚ö†Ô∏è {self.name}: –í—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø—É—Å—Ç—ã–µ –ø–æ—Å–ª–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏")
            self.is_fitted = False
            return {
                "module": self.name,
                "status": "error",
                "message": "All documents empty after preprocessing",
            }

        # –°–æ–∑–¥–∞–µ–º/–æ–±–Ω–æ–≤–ª—è–µ–º BM25 –∏–Ω–¥–µ–∫—Å
        from rank_bm25 import BM25Okapi

        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –º—è–≥–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            self.bm25 = BM25Okapi(
                processed_docs, k1=1.2, b=0.75  # –ë–æ–ª–µ–µ –º—è–≥–∫–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
            )
            self.is_fitted = True
            self.total_terms = sum(len(doc) for doc in processed_docs)

            print(
                f"‚úÖ {self.name}: –ò–Ω–¥–µ–∫—Å –ø–æ—Å—Ç—Ä–æ–µ–Ω. –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(self.documents)}, –¢–µ—Ä–º–∏–Ω–æ–≤: {self.total_terms}"
            )

            return {
                "module": self.name,
                "status": "success",
                "added": len(documents),
                "total": len(self.documents),
                "total_terms": self.total_terms,
            }

        except Exception as e:
            print(f"‚ùå {self.name}: –û—à–∏–±–∫–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞: {e}")
            self.is_fitted = False
            return {"module": self.name, "status": "error", "message": str(e)}

    def fit(self, documents: List[str], ids: Optional[List[str]] = None) -> Dict:
        """–ê–ª–∏–∞—Å –¥–ª—è add_documents (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å –¥—Ä—É–≥–∏–º–∏ –º–æ–¥—É–ª—è–º–∏)"""
        return self.add_documents(documents, ids)

    def search(self, query: str, top_k: int = 5) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É"""
        if not self.is_fitted or not self.bm25 or len(self.documents) == 0:
            print(
                f"‚ö†Ô∏è {self.name}: –ü–æ–∏—Å–∫ –Ω–µ–≤–æ–∑–º–æ–∂–µ–Ω. is_fitted={self.is_fitted}, bm25={self.bm25 is not None}, docs={len(self.documents)}"
            )
            return []

        try:
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å
            processed_query = self._preprocess_text(query)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∑–∞–ø—Ä–æ—Å –Ω–µ –ø—É—Å—Ç–æ–π –ø–æ—Å–ª–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
            if not processed_query:
                print(f"‚ö†Ô∏è {self.name}: –ó–∞–ø—Ä–æ—Å '{query}' –ø—É—Å—Ç–æ–π –ø–æ—Å–ª–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏")
                return []

            # –ü–æ–ª—É—á–∞–µ–º —Å–∫–æ—Ä—ã
            raw_scores = self.bm25.get_scores(processed_query)

            # üî• –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å–∫–æ—Ä—ã –∫ [0, 1]
            if len(raw_scores) > 0:
                # 1. –ò–∑–±–∞–≤–ª—è–µ–º—Å—è –æ—Ç –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π (—Å–¥–≤–∏–≥–∞–µ–º –≤—Å–µ –≤–≤–µ—Ä—Ö)
                min_score = np.min(raw_scores)
                if min_score < 0:
                    shifted_scores = raw_scores - min_score + 0.1
                else:
                    shifted_scores = raw_scores + 0.1  # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ

                # 2. –ü—Ä–∏–º–µ–Ω—è–µ–º –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–ª—è —Ä–∞—Å—Ç—è–≥–∏–≤–∞–Ω–∏—è
                # log1p(x) = log(1 + x) - –∏–∑–±–µ–≥–∞–µ–º log(0)
                log_scores = np.log1p(shifted_scores * 10)  # –£–º–Ω–æ–∂–∞–µ–º –Ω–∞ 10 –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è

                # 3. Min-max –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫ [0, 1]
                max_log = np.max(log_scores)
                min_log = np.min(log_scores)

                if max_log > min_log:
                    normalized_scores = (log_scores - min_log) / (max_log - min_log)
                else:
                    # –ï—Å–ª–∏ –≤—Å–µ —Å–∫–æ—Ä—ã —Ä–∞–≤–Ω—ã, –∑–∞–¥–∞–µ–º –±–∞–∑–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                    normalized_scores = np.ones_like(log_scores) * 0.5

                # 4. –°–∏–≥–º–æ–∏–¥–∞ –¥–ª—è –±–æ–ª–µ–µ —á–µ—Ç–∫–æ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
                # scores = 1 / (1 + np.exp(-normalized_scores * 6 + 3))
                scores = normalized_scores  # –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞–ø—Ä—è–º—É—é

            else:
                scores = np.array([])

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π —Å–∫–æ—Ä
            if len(scores) > 0 and np.max(scores) > 0:
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é
                top_indices = np.argsort(scores)[::-1][:top_k]

                # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–º–∏ —Å–∫–æ—Ä–∞–º–∏
                results = []
                for idx in top_indices:
                    if scores[idx] > 0.01:  # –ù–µ–±–æ–ª—å—à–æ–π –ø–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                        results.append(
                            {
                                "id": self.ids[idx],
                                "content": self.documents[idx],
                                "score": float(scores[idx]),  # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Å–∫–æ—Ä 0-1
                                "raw_score": float(raw_scores[idx]),  # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π BM25 —Å–∫–æ—Ä
                                "module": self.name,
                                "module_type": "bm25",
                            }
                        )

                print(
                    f"‚úÖ {self.name}: –ü–æ–∏—Å–∫ '{query}' -> –Ω–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Å–∫–æ—Ä—ã)"
                )
                return results

            else:
                # üî• –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-3 –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–∞–∂–µ —Å –Ω–∏–∑–∫–∏–º–∏ —Å–∫–æ—Ä–∞–º–∏
                top_indices = np.argsort(raw_scores)[::-1][:top_k]
                results = []
                for idx in top_indices:
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–∞ –ª–µ—Ç—É –¥–ª—è fallback
                    raw_score = raw_scores[idx]
                    norm_score = max(0.01, min(0.1, raw_score / 100)) if raw_score > 0 else 0.01

                    results.append(
                        {
                            "id": self.ids[idx],
                            "content": self.documents[idx],
                            "score": norm_score,  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Å–∫–æ—Ä
                            "raw_score": float(raw_scores[idx]),
                            "module": self.name,
                            "module_type": "bm25",
                            "note": "low_confidence",
                        }
                    )

                if results:
                    print(
                        f"‚ö†Ô∏è {self.name}: –ü–æ–∏—Å–∫ '{query}' -> –Ω–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–Ω–∏–∑–∫–∏–µ —Å–∫–æ—Ä—ã)"
                    )
                    return results
                else:
                    print(f"‚ö†Ô∏è {self.name}: –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ '{query}'")
                    return []

        except Exception as e:
            print(f"‚ùå {self.name}: –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ '{query}': {e}")
            return []

    def get_info(self):
        return {
            "name": self.name,
            "type": "bm25",
            "language": self.language,
            "total_documents": len(self.documents),
            "total_terms": self.total_terms,
            "is_fitted": self.is_fitted,
        }

    def clear(self) -> Dict:
        self.documents = []
        self.ids = []
        self.bm25 = None
        self.is_fitted = False
        self.total_terms = 0
        return {"module": self.name, "status": "cleared"}

    def save(self, path: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –º–æ–¥—É–ª—è"""
        module_path = os.path.join(path, self.name)
        os.makedirs(module_path, exist_ok=True)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
        data = {
            "documents": self.documents,
            "ids": self.ids,
            "language": self.language,
            "total_terms": self.total_terms,
            "is_fitted": self.is_fitted,
        }

        with open(os.path.join(module_path, "data.json"), "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º BM25 –º–æ–¥–µ–ª—å (–µ—Å–ª–∏ –µ—Å—Ç—å)
        if self.bm25:
            with open(os.path.join(module_path, "bm25.pkl"), "wb") as f:
                pickle.dump(self.bm25, f)

    def load(self, path: str) -> bool:
        """–ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –º–æ–¥—É–ª—è"""
        module_path = os.path.join(path, self.name)

        if not os.path.exists(module_path):
            return False

        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            with open(os.path.join(module_path, "data.json"), "r", encoding="utf-8") as f:
                data = json.load(f)

            self.documents = data["documents"]
            self.ids = data["ids"]
            self.language = data.get("language", "multilingual")
            self.total_terms = data.get("total_terms", 0)
            self.is_fitted = data.get("is_fitted", False)

            # –ó–∞–≥—Ä—É–∂–∞–µ–º BM25 –º–æ–¥–µ–ª—å
            bm25_path = os.path.join(module_path, "bm25.pkl")
            if os.path.exists(bm25_path):
                with open(bm25_path, "rb") as f:
                    self.bm25 = pickle.load(f)

            print(
                f"‚úÖ {self.name}: –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, is_fitted={self.is_fitted}"
            )
            return True

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥—É–ª—è {self.name}: {e}")
            return False
