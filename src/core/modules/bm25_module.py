"""
bm25_module.py - BM25 –º–æ–¥—É–ª—å –¥–ª—è –ø–æ–∏—Å–∫–∞ (–º—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã–π)
"""

import numpy as np
from typing import List, Dict, Any, Optional
import json
import pickle
import os
import nltk
from src.core.base import BaseSearchModule


class BM25Module(BaseSearchModule):

    def __init__(self, name: str = "bm25", language: str = "multilingual"):
        self.name = name
        self.language = language
        self.is_fitted = False
        self.documents = []
        self.total_terms = 0
        self.ids = []
        self.bm25 = None

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —è–∑—ã–∫–æ–≤
        nltk.download("punkt", quiet=True)
        nltk.download("stopwords", quiet=True)
        from nltk.corpus import stopwords

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏–∑ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–æ–≤
        self.stop_words = set()
        for lang in ["english", "russian", "french", "spanish", "german"]:
            try:
                self.stop_words.update(stopwords.words(lang))
            except:
                pass

        # –£–±–∏—Ä–∞–µ–º –≤–∞–∂–Ω—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Å—Ç–æ–ø-—Å–ª–æ–≤
        important_words = {
            "war",
            "world",
            "technology",
            "python",
            "intelligence",
            "bitcoin",
            "blockchain",
            "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π",
            "–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç",
            "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è",
        }
        self.stop_words = {w for w in self.stop_words if w not in important_words}

    def _preprocess_text(self, text: str) -> List[str]:
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–µ–∑ —Å—Ç–µ–º–º–∏–Ω–≥–∞"""
        try:
            tokens = nltk.word_tokenize(text.lower())
        except:
            import re

            tokens = re.findall(r"\b\w+\b", text.lower())

        # –ë–æ–ª–µ–µ –º—è–≥–∫–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
        processed = []
        for token in tokens:
            if len(token) > 1 and token not in self.stop_words and not token.isdigit():
                processed.append(token)

        return processed

    def add_documents(self, documents: List[str], ids: Optional[List[str]] = None) -> Dict:
        """–î–æ–±–∞–≤–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ —Å—Ç—Ä–æ–∏—Ç –∏–Ω–¥–µ–∫—Å"""
        if ids is None:
            ids = [
                f"{self.name}_{i}"
                for i in range(len(self.documents), len(self.documents) + len(documents))
            ]

        # –ï—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã —É–∂–µ –µ—Å—Ç—å, –æ—á–∏—â–∞–µ–º
        if self.documents:
            self.documents = []
            self.ids = []

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        self.documents.extend(documents)
        self.ids.extend(ids)

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        processed_docs = [self._preprocess_text(doc) for doc in self.documents]

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –µ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        if not processed_docs or all(len(doc) == 0 for doc in processed_docs):
            print(f"‚ö†Ô∏è {self.name}: –í—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø—É—Å—Ç—ã–µ –ø–æ—Å–ª–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏")
            self.is_fitted = False
            return {
                "module": self.name,
                "status": "error",
                "message": "All documents empty after preprocessing",
            }

        # –°–æ–∑–¥–∞–µ–º/–æ–±–Ω–æ–≤–ª—è–µ–º BM25 –∏–Ω–¥–µ–∫—Å
        from rank_bm25 import BM25Okapi

        try:
            # üî• –®–ê–ì 2: –û–ü–¢–ò–ú–ê–õ–¨–ù–´–ï –ü–ê–†–ê–ú–ï–¢–†–´ –î–õ–Ø –ë–û–õ–¨–®–û–ô –ö–û–õ–õ–ï–ö–¶–ò–ò
            self.bm25 = BM25Okapi(
                processed_docs,
                k1=2.5,  # –£–≤–µ–ª–∏—á–∏—Ç—å –¥–ª—è –±–æ–ª—å—à–∏—Ö –∫–æ–ª–ª–µ–∫—Ü–∏–π (–±—ã–ª–æ 1.2)
                b=0.9,  # –£–≤–µ–ª–∏—á–∏—Ç—å –¥–ª—è –ª—É—á—à–µ–≥–æ —É—á–µ—Ç–∞ –¥–ª–∏–Ω—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞
            )
            self.is_fitted = True
            self.total_terms = sum(len(doc) for doc in processed_docs)

            print(f"‚úÖ {self.name}: –ò–Ω–¥–µ–∫—Å –ø–æ—Å—Ç—Ä–æ–µ–Ω —Å k1=2.5, b=0.9")

            return {
                "module": self.name,
                "status": "success",
                "added": len(documents),
                "total": len(self.documents),
                "total_terms": self.total_terms,
            }

        except Exception as e:
            print(f"‚ùå {self.name}: –û—à–∏–±–∫–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞: {e}")
            self.is_fitted = False
            return {"module": self.name, "status": "error", "message": str(e)}

    def fit(self, documents: List[str], ids: Optional[List[str]] = None) -> Dict:
        """–ê–ª–∏–∞—Å –¥–ª—è add_documents (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å –¥—Ä—É–≥–∏–º–∏ –º–æ–¥—É–ª—è–º–∏)"""
        return self.add_documents(documents, ids)

    def search(self, query: str, top_k: int = 5) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É"""
        if not self.is_fitted or not self.bm25 or len(self.documents) == 0:
            print(
                f"‚ö†Ô∏è {self.name}: –ü–æ–∏—Å–∫ –Ω–µ–≤–æ–∑–º–æ–∂–µ–Ω. is_fitted={self.is_fitted}, bm25={self.bm25 is not None}, docs={len(self.documents)}"
            )
            return []

        try:
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å
            processed_query = self._preprocess_text(query)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∑–∞–ø—Ä–æ—Å –Ω–µ –ø—É—Å—Ç–æ–π –ø–æ—Å–ª–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
            if not processed_query:
                print(f"‚ö†Ô∏è {self.name}: –ó–∞–ø—Ä–æ—Å '{query}' –ø—É—Å—Ç–æ–π –ø–æ—Å–ª–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏")
                return []

            # –ü–æ–ª—É—á–∞–µ–º —Å–∫–æ—Ä—ã
            raw_scores = self.bm25.get_scores(processed_query)

            # üî• –®–ê–ì 1: –ü–†–û–°–¢–ê–Ø –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø
            if len(raw_scores) > 0:
                # –ü–æ–ª—É—á–∞–µ–º min –∏ max
                min_score = np.min(raw_scores)
                max_score = np.max(raw_scores)

                # –ï—Å–ª–∏ –≤—Å–µ scores –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ (—Ä–µ–¥–∫–∏–π —Å–ª—É—á–∞–π)
                if max_score - min_score < 1e-6:
                    normalized_scores = np.ones_like(raw_scores) * 0.5
                else:
                    # –ü—Ä–æ—Å—Ç–∞—è min-max –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫ [0, 1]
                    normalized_scores = (raw_scores - min_score) / (max_score - min_score)

                scores = normalized_scores
            else:
                scores = np.array([])

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π —Å–∫–æ—Ä
            if len(scores) > 0 and np.max(scores) > 0:
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é
                top_indices = np.argsort(scores)[::-1][:top_k]

                # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                results = []
                for idx in top_indices:
                    if scores[idx] > 0.01:  # –ù–µ–±–æ–ª—å—à–æ–π –ø–æ—Ä–æ–≥
                        results.append(
                            {
                                "id": self.ids[idx],
                                "content": self.documents[idx],
                                "score": float(scores[idx]),  # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Å–∫–æ—Ä 0-1
                                "raw_score": float(raw_scores[idx]),
                                "module": self.name,
                                "module_type": "bm25",
                            }
                        )

                print(f"‚úÖ {self.name}: –ü–æ–∏—Å–∫ '{query}' -> {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                return results
            else:
                # Fallback: –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-3 —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ —Å–∫–æ—Ä–∞–º–∏
                top_indices = np.argsort(raw_scores)[::-1][: min(top_k, 3)]
                results = []
                for idx in top_indices:
                    results.append(
                        {
                            "id": self.ids[idx],
                            "content": self.documents[idx],
                            "score": 0.05,  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π confidence
                            "raw_score": float(raw_scores[idx]),
                            "module": self.name,
                            "module_type": "bm25",
                            "note": "low_confidence",
                        }
                    )

                if results:
                    print(f"‚ö†Ô∏è {self.name}: –ù–∏–∑–∫–∏–µ —Å–∫–æ—Ä—ã –¥–ª—è '{query}'")
                    return results
                else:
                    return []

        except Exception as e:
            print(f"‚ùå {self.name}: –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return []

    def get_info(self):
        return {
            "name": self.name,
            "type": "bm25",
            "language": self.language,
            "total_documents": len(self.documents),
            "total_terms": self.total_terms,
            "is_fitted": self.is_fitted,
        }

    def clear(self) -> Dict:
        self.documents = []
        self.ids = []
        self.bm25 = None
        self.is_fitted = False
        self.total_terms = 0
        return {"module": self.name, "status": "cleared"}

    def save(self, path: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –º–æ–¥—É–ª—è"""
        module_path = os.path.join(path, self.name)
        os.makedirs(module_path, exist_ok=True)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
        data = {
            "documents": self.documents,
            "ids": self.ids,
            "language": self.language,
            "total_terms": self.total_terms,
            "is_fitted": self.is_fitted,
        }

        with open(os.path.join(module_path, "data.json"), "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º BM25 –º–æ–¥–µ–ª—å (–µ—Å–ª–∏ –µ—Å—Ç—å)
        if self.bm25:
            with open(os.path.join(module_path, "bm25.pkl"), "wb") as f:
                pickle.dump(self.bm25, f)

    def load(self, path: str) -> bool:
        """–ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –º–æ–¥—É–ª—è"""
        module_path = os.path.join(path, self.name)

        if not os.path.exists(module_path):
            return False

        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            with open(os.path.join(module_path, "data.json"), "r", encoding="utf-8") as f:
                data = json.load(f)

            self.documents = data["documents"]
            self.ids = data["ids"]
            self.language = data.get("language", "multilingual")
            self.total_terms = data.get("total_terms", 0)
            self.is_fitted = data.get("is_fitted", False)

            # –ó–∞–≥—Ä—É–∂–∞–µ–º BM25 –º–æ–¥–µ–ª—å
            bm25_path = os.path.join(module_path, "bm25.pkl")
            if os.path.exists(bm25_path):
                with open(bm25_path, "rb") as f:
                    self.bm25 = pickle.load(f)

            print(
                f"‚úÖ {self.name}: –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, is_fitted={self.is_fitted}"
            )
            return True

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥—É–ª—è {self.name}: {e}")
            return False
